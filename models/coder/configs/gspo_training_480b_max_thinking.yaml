# GSPO Training for Qwen3-Coder-480B-A35B-Thinking (Zen Coder Max Thinking)
# State-of-the-art code generation with visible reasoning
# Based on Alibaba's GSPO: https://arxiv.org/abs/2507.18071

### Model configuration
model_name_or_path: Qwen/Qwen3-Coder-480B-A35B-Thinking  # Max Thinking variant
template: qwen3-coder-thinking
model_type: coder-moe-max-thinking

### Training method
stage: gspo
finetuning_type: lora
use_ref_model: true

### GSPO specific parameters (optimized for max reasoning)
gspo_group_size: 4              # Small groups for complex reasoning
gspo_beta: 0.18                 # Higher for thinking stability
gspo_clip_epsilon: 0.3
gspo_sequence_level: true       # Critical for reasoning chains
gspo_normalize_rewards: true
gspo_moe_stabilization: true    # MoE with 128 experts, 8 active

### Thinking-specific configuration
enable_thinking_mode: true      # Enable deep reasoning
thinking_max_length: 65536      # Extended thinking capacity
thinking_temperature: 0.8       # Exploration in reasoning
separate_thinking_loss: true    # Independent thinking optimization
thinking_dropout: 0.1           # Prevent overfitting on reasoning

### Dataset configuration
dataset: code_alpaca_thinking   # Dataset with reasoning traces
dataset_dir: data
cutoff_len: 262144             # Full 256K context for complex problems
overwrite_cache: true
preprocessing_num_workers: 16

### LoRA configuration (high capacity)
lora_rank: 128                 # Maximum rank for complex reasoning
lora_alpha: 256
lora_dropout: 0.05
lora_target: all
use_rslora: true               # Rank-stabilized LoRA

### Training hyperparameters
per_device_train_batch_size: 1
gradient_accumulation_steps: 32
learning_rate: 1e-5
num_train_epochs: 2
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 7200              # Longer timeout for large model

### Generation configuration
max_new_tokens: 32768          # Extended generation for reasoning
temperature: 0.7
top_p: 0.95
do_sample: true
repetition_penalty: 1.05       # Prevent reasoning loops

### Evaluation
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 50

### Logging and saving
logging_steps: 5
save_steps: 250
save_total_limit: 2
output_dir: saves/qwen3-coder-480b-thinking

### DeepSpeed configuration
deepspeed: configs/deepspeed/ds_z3_offload.json

### Notes
# Maximum capability thinking model
# Shows complete reasoning process for complex problems
# Best for research, complex system design, and algorithm development
# Requires significant compute resources (80GB+ VRAM recommended)