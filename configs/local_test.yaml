# Local test configuration for quick GSPO demo
# Using small OPT-125M model for fast training

### Model configuration
model_name_or_path: facebook/opt-125m
template: default

### Training method
stage: sft  # Using SFT for quick demo (GSPO requires more setup)
finetuning_type: lora
do_train: true

### LoRA configuration
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1
lora_target: all

### Dataset configuration
dataset: alpaca_gpt4_en
cutoff_len: 256
overwrite_cache: true
max_samples: 50  # Small sample for quick test

### Training hyperparameters
per_device_train_batch_size: 2
gradient_accumulation_steps: 1
learning_rate: 5.0e-5
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.1

### Output configuration
output_dir: saves/local-test
overwrite_output_dir: true
logging_steps: 5
save_steps: 50
save_total_limit: 1

### Reporting
report_to: none  # Disable external reporting for local test

### System
preprocessing_num_workers: 1